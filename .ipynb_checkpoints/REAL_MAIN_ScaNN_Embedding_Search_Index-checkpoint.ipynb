{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nackerboss/SCANNN/blob/main/REAL_MAIN_ScaNN_Embedding_Search_Index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "w08aKpa29OBM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scann in ./scann-env/lib/python3.12/site-packages (1.4.2)\n",
      "Requirement already satisfied: sentence-transformers in ./scann-env/lib/python3.12/site-packages (5.1.2)\n",
      "Requirement already satisfied: datasets in ./scann-env/lib/python3.12/site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy~=2.0 in ./scann-env/lib/python3.12/site-packages (from scann) (2.3.4)\n",
      "Requirement already satisfied: protobuf in ./scann-env/lib/python3.12/site-packages (from scann) (6.33.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in ./scann-env/lib/python3.12/site-packages (from sentence-transformers) (4.57.1)\n",
      "Requirement already satisfied: tqdm in ./scann-env/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./scann-env/lib/python3.12/site-packages (from sentence-transformers) (2.9.0)\n",
      "Requirement already satisfied: scikit-learn in ./scann-env/lib/python3.12/site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in ./scann-env/lib/python3.12/site-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in ./scann-env/lib/python3.12/site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in ./scann-env/lib/python3.12/site-packages (from sentence-transformers) (12.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in ./scann-env/lib/python3.12/site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in ./scann-env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./scann-env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./scann-env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./scann-env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.10.23)\n",
      "Requirement already satisfied: requests in ./scann-env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./scann-env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./scann-env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./scann-env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./scann-env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./scann-env/lib/python3.12/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./scann-env/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in ./scann-env/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in ./scann-env/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in ./scann-env/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./scann-env/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./scann-env/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in ./scann-env/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in ./scann-env/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in ./scann-env/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in ./scann-env/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./scann-env/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./scann-env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./scann-env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./scann-env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./scann-env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./scann-env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./scann-env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./scann-env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./scann-env/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./scann-env/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: setuptools in ./scann-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./scann-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./scann-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./scann-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./scann-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./scann-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./scann-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./scann-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./scann-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./scann-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./scann-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./scann-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./scann-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./scann-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./scann-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./scann-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./scann-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./scann-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./scann-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in ./scann-env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./scann-env/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./scann-env/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./scann-env/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./scann-env/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./scann-env/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./scann-env/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./scann-env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./scann-env/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./scann-env/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scann sentence-transformers datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NrNu6kxo_qwi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ph879PTv9NK-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading public dataset (ag_news) subset...\n",
      "Generating embeddings for 5000 items...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [00:33<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing embeddings...\n",
      "Generating embeddings for 20 items...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.57it/s]\n",
      "I0000 00:00:1762237923.870735   13532 partitioner_factory_base.cc:58] Size of sampled dataset for training partition: 4000\n",
      "I0000 00:00:1762237923.951447   13532 kmeans_tree_partitioner_utils.h:90] PartitionerFactory ran in 80.666928ms.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing embeddings...\n",
      "\n",
      "Dataset Ready. Shape: (5000, 384)\n",
      "Test Query Set Shape: (20, 384)\n",
      "First dataset entry (Index Training Data): Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "\n",
      "--- 4. Building ScaNN Optimized Searcher (Trained on 5000 examples) ---\n",
      "ScaNN optimized index built successfully.\n"
     ]
    }
   ],
   "source": [
    "# This script demonstrates the full workflow using a public dataset:\n",
    "# 1. Install necessary libraries (scann, sentence-transformers, datasets).\n",
    "# 2. Load a public text dataset (Hugging Face ag_news) for both training (index) and testing (queries).\n",
    "# 3. Generate and normalize vector embeddings for both sets.\n",
    "# 4. Build a high-performance ScaNN index on the training set.\n",
    "# 5. Build a Brute-Force searcher and compute recall using the test set queries.\n",
    "# 6. Run a sample similarity query.\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Installation (Run this in a separate Colab cell first!) ---\n",
    "# Note: You now need 'datasets' installed.\n",
    "# !pip install scann sentence-transformers datasets\n",
    "\n",
    "try:\n",
    "    import scann\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from datasets import load_dataset # New import for public dataset\n",
    "except ImportError:\n",
    "    print(\"----------------------------------------------------------------------\")\n",
    "    print(\"üö® ERROR: Please run the following command in a separate Colab cell \")\n",
    "    print(\"and restart the runtime before running this code:\")\n",
    "    print(\"!pip install scann sentence-transformers datasets\")\n",
    "    print(\"----------------------------------------------------------------------\")\n",
    "    exit()\n",
    "\n",
    "# --- Utility Function for Recall Calculation (Provided by user) ---\n",
    "\n",
    "def compute_recall(neighbors, true_neighbors):\n",
    "    \"\"\"\n",
    "    Computes recall @k by comparing the results of the approximate search\n",
    "    (neighbors) against the exact search (true_neighbors).\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    # Iterate through query results, comparing the approximate set against the true set\n",
    "    for gt_row, row in zip(true_neighbors, neighbors):\n",
    "        # Count the number of common elements (true positives)\n",
    "        total += np.intersect1d(gt_row, row).shape[0]\n",
    "\n",
    "    # Recall is (True Positives) / (Total True Neighbors)\n",
    "    return total / true_neighbors.size\n",
    "\n",
    "# --- 2. Setup and Data Loading ---\n",
    "\n",
    "MODEL_NAME = 'all-MiniLM-L6-v2'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "embedding_model = SentenceTransformer(MODEL_NAME, device=device)\n",
    "\n",
    "# Load a public dataset (ag_news) and take a manageable subset for demonstration\n",
    "print(\"Loading public dataset (ag_news) subset...\")\n",
    "try:\n",
    "    # Load the training split (used for building the ScaNN index)\n",
    "    ag_news_dataset_train = load_dataset('ag_news', split='train[:5000]')\n",
    "    dataset = ag_news_dataset_train['text']\n",
    "\n",
    "    # Load the test split (used for generating test queries for recall calculation)\n",
    "    ag_news_dataset_test = load_dataset('ag_news', split='test[:20]')\n",
    "    test_dataset_text = ag_news_dataset_test['text']\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ag_news dataset: {e}\")\n",
    "    # Fallback to the original small dataset if loading fails\n",
    "    dataset = [\n",
    "        \"The sun rises in the east every morning.\",\n",
    "        \"A computer uses a central processing unit for core tasks.\",\n",
    "        \"Cats and dogs are common household pets.\",\n",
    "        \"A feline companion enjoying a nap on the sofa.\",\n",
    "        \"The central processing unit is the brain of any modern machine.\",\n",
    "        \"Tomorrow's forecast predicts clear skies and warm weather.\"\n",
    "    ]\n",
    "    test_dataset_text = dataset # Use the same small data for queries if primary fails\n",
    "\n",
    "\n",
    "# The queries we will use to search the dataset\n",
    "query_text_1 = \"The main component of a PC is the CPU.\"\n",
    "query_text_2 = \"What is the weather like at dawn?\"\n",
    "query_text_3 = \"Football match results from the weekend.\"\n",
    "\n",
    "def generate_and_normalize(data):\n",
    "    \"\"\"Generates embeddings and performs L2 normalization.\"\"\"\n",
    "    print(f\"Generating embeddings for {len(data)} items...\")\n",
    "\n",
    "    # 3.1 Generate embeddings (returns a numpy array)\n",
    "    embeddings = embedding_model.encode(\n",
    "        data,\n",
    "        convert_to_tensor=False,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    # 3.2 L2 Normalization (Crucial for ScaNN dot product or angular similarity)\n",
    "    print(\"Normalizing embeddings...\")\n",
    "    normalized_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "    return normalized_embeddings, embeddings.shape[1]\n",
    "\n",
    "normalized_dataset_embeddings, embedding_dim = generate_and_normalize(dataset)\n",
    "\n",
    "normalized_test_embeddings, _ = generate_and_normalize(test_dataset_text)\n",
    "\n",
    "print(f\"\\nDataset Ready. Shape: {normalized_dataset_embeddings.shape}\")\n",
    "print(f\"Test Query Set Shape: {normalized_test_embeddings.shape}\")\n",
    "print(f\"First dataset entry (Index Training Data): {dataset[0]}\")\n",
    "\n",
    "\n",
    "# --- 4. Building the ScaNN Index (Optimized for 5000 vectors) ---\n",
    "\n",
    "print(\"\\n--- 4. Building ScaNN Optimized Searcher (Trained on 5000 examples) ---\")\n",
    "\n",
    "# The maximum number of neighbors to retrieve (top-k)\n",
    "K_NEIGHBORS = 5\n",
    "REORDER_NEIGHBORS = 50 # Reduced reorder candidates for speedier demo\n",
    "\n",
    "# 4.1. Initialize the ScaNN builder\n",
    "# Arguments: (dataset, k, distance_metric)\n",
    "builder = scann.scann_ops_pybind.builder(\n",
    "    normalized_dataset_embeddings,\n",
    "    K_NEIGHBORS,\n",
    "    \"dot_product\"\n",
    ")\n",
    "\n",
    "# 4.2. Configure the Tree (Partitioning) stage\n",
    "tree_configured = builder.tree(\n",
    "    num_leaves=500,\n",
    "    num_leaves_to_search=50,\n",
    "    training_sample_size=4000\n",
    ")\n",
    "\n",
    "# 4.3. Configure Asymmetric Hashing (AH) for scoring\n",
    "ah_configured = tree_configured.score_ah(\n",
    "    8, # Number of dimensions per subvector\n",
    "    anisotropic_quantization_threshold=0.2\n",
    ")\n",
    "\n",
    "# 4.4. Configure the Reordering (Refinement) stage\n",
    "reorder_configured = ah_configured.reorder(REORDER_NEIGHBORS)\n",
    "\n",
    "# 4.5. Finalize and build the searcher\n",
    "searcher = reorder_configured.build()\n",
    "\n",
    "print(\"ScaNN optimized index built successfully.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "P-IYIPbMCrdT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 5. Computing Recall (ScaNN vs. Brute Force) ---\n",
      "1. Running Brute-Force search on 20 test queries...\n",
      "2. Running Optimized ScaNN search...\n",
      "\n",
      "‚úÖ Recall @5 for 20 queries from the TEST split: 93.00%\n",
      "This value indicates the percentage of exact nearest neighbors found by the approximate searcher.\n",
      "\n",
      "Searching with query: 'The main component of a PC is the CPU.'\n",
      "\n",
      "Top 5 results found:\n",
      "  Rank 1:\n",
      "3926\n",
      "    Text: Intel Chips In for New Gateway PCs Desktops will be available at several retailers, including CompUSA.\n",
      "    Similarity (Dot Product): 0.4512\n",
      "    Dataset Index: 3926\n",
      "  Rank 2:\n",
      "4704\n",
      "    Text: AMD #39;s new budget processors AMD #39;s new Sempron range of desktop and notebook CPUs is targeted squarely at Intel #39;s competing Celeron family. \n",
      "    Similarity (Dot Product): 0.4472\n",
      "    Dataset Index: 4704\n",
      "  Rank 3:\n",
      "2698\n",
      "    Text: New PC Is Created Just for Teenagers This isn't your typical, humdrum, slate-colored computer. Not only is the PC known as the hip-e almost all white, but its screen and keyboard are framed in fuzzy pink fur. Or a leopard skin design. Or a graffiti-themed pattern.\n",
      "    Similarity (Dot Product): 0.4422\n",
      "    Dataset Index: 2698\n",
      "  Rank 4:\n",
      "1814\n",
      "    Text: New PC Is Created Just for Teenagers (AP) AP - This isn't your typical, humdrum, slate-colored computer. Not only is the PC known as the hip-e almost all white, but its screen and keyboard are framed in fuzzy pink fur. Or a leopard skin design. Or a graffiti-themed pattern.\n",
      "    Similarity (Dot Product): 0.4302\n",
      "    Dataset Index: 1814\n",
      "  Rank 5:\n",
      "2067\n",
      "    Text: New Computer Is Created Just for Teenagers This isn't your typical, humdrum, slate-colored computer. Not only is the PC known as the hip-e almost all white, but its screen and keyboard are framed in fuzzy pink fur. Or a leopard skin design. Or a graffiti-themed pattern.\n",
      "    Similarity (Dot Product): 0.4210\n",
      "    Dataset Index: 2067\n",
      "\n",
      "Searching with query: 'What is the weather like at dawn?'\n",
      "\n",
      "Top 5 results found:\n",
      "  Rank 1:\n",
      "214\n",
      "    Text: Forecast: Plenty of Activity On the Weather Blog Front If it's possible to love something just because it could visit torrents upon the Washington region, fling hail from the skies and swell streams into rivers, then Jason Samenow is smitten.\n",
      "    Similarity (Dot Product): 0.3631\n",
      "    Dataset Index: 214\n",
      "  Rank 2:\n",
      "256\n",
      "    Text: \"Unusually Good\" Meteor Shower Expected Tonight Tonight's annual Perseid meteor shower is likely to be a spectacular show of shooting stars zipping across the night sky, according to astronomers.\n",
      "    Similarity (Dot Product): 0.3584\n",
      "    Dataset Index: 256\n",
      "  Rank 3:\n",
      "101\n",
      "    Text: Perseid Meteor Shower Peaks Overnight (SPACE.com) SPACE.com - A fine display of shooting stars is underway and peaks overnight Wednesday into early Thursday morning. Astronomers expect the 2004 Perseid meteor shower to be one of the best versions of the annual event in several years.\n",
      "    Similarity (Dot Product): 0.3566\n",
      "    Dataset Index: 101\n",
      "  Rank 4:\n",
      "3873\n",
      "    Text: Torrential British Rain Set to Cause More Chaos  LONDON (Reuters) - After torrential rain caused flash  flooding in southern England and a landslide in Scotland this  week, meteorologists on Thursday forecast more heavy downpours  and urged people to heed bad weather warnings.\n",
      "    Similarity (Dot Product): 0.3488\n",
      "    Dataset Index: 3873\n",
      "  Rank 5:\n",
      "897\n",
      "    Text: News: Future Heat Waves: More Severe, More Frequent, and Longer Lasting Heat waves in Chicago, Paris, and elsewhere in North America and Europe will become more intense, more frequent, and longer lasting in the 21st century, according to a new modeling study by two scientists at the National Center for Atmospheric Research (NCAR). (NCAR press release)\n",
      "    Similarity (Dot Product): 0.3414\n",
      "    Dataset Index: 897\n",
      "\n",
      "Searching with query: 'Football match results from the weekend.'\n",
      "\n",
      "Top 5 results found:\n",
      "  Rank 1:\n",
      "4956\n",
      "    Text: Live on Sky Sports 2, 5pm (KO 5.15pm) This is, in my opinion, one of the biggest games Manchester United have played for years. It may sound strange ahead of what is only the second game of the season, but don #39;t think this is one they can afford to lose. \n",
      "    Similarity (Dot Product): 0.4724\n",
      "    Dataset Index: 4956\n",
      "  Rank 2:\n",
      "2921\n",
      "    Text: VARSITY MATCH Oxford regained the MMC Trophy with a hard-fought 18-11 win over Cambridge in the Varsity match at Twickenham. Jon Fennell kicked Oxford into an early lead, but a well-worked try from Fergie Gladstone saw Cambridge hit back.\n",
      "    Similarity (Dot Product): 0.4374\n",
      "    Dataset Index: 2921\n",
      "  Rank 3:\n",
      "2944\n",
      "    Text: Late try gives Oxford win Ross Lavery scored a try two minutes from the end as Oxford University defeated Cambridge 18-11 at Twickenham on Tuesday in another tight finish to the annual University rugby match.\n",
      "    Similarity (Dot Product): 0.4197\n",
      "    Dataset Index: 2944\n",
      "  Rank 4:\n",
      "4383\n",
      "    Text: Precision and Pressure in a Classic Stadium THENS, Aug. 19  The two finalists in the men #39;s archery strode out this afternoon and took their positions side-by-by side between the twin columns topped with the two-faced busts of Hermes that mark the start and finish lines in the ...\n",
      "    Similarity (Dot Product): 0.3870\n",
      "    Dataset Index: 4383\n",
      "  Rank 5:\n",
      "1695\n",
      "    Text: Aussies Battle Americans to 1-1 Tie (AP) AP - After 17 years, the Matildas finally caught up to the U.S. women's soccer team. Joanne Peters' 12-yard header in the 82nd minute gave Australia a 1-1 tie Tuesday with the United States, breaking a 15-game Aussie losing streak that dates to the teams' first meeting in 1987.\n",
      "    Similarity (Dot Product): 0.3796\n",
      "    Dataset Index: 1695\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Computing Recall (ScaNN vs. Brute Force) ---\n",
    "\n",
    "print(\"\\n--- 5. Computing Recall (ScaNN vs. Brute Force) ---\")\n",
    "\n",
    "# 5.1. Create a Brute-Force ScaNN searcher (no tree, no quantization)\n",
    "# This will find the mathematically exact nearest neighbors.\n",
    "bruteforce_searcher = scann.scann_ops_pybind.builder(\n",
    "    normalized_dataset_embeddings,\n",
    "    K_NEIGHBORS,\n",
    "    \"dot_product\"\n",
    ").score_brute_force().build()\n",
    "\n",
    "# 5.2. Define Test Queries (using a subset of the official test split as queries)\n",
    "# Limit the number of test queries for faster recall computation\n",
    "MAX_TEST_QUERIES = 500\n",
    "NUM_RECALL_QUERIES = min(MAX_TEST_QUERIES, len(normalized_test_embeddings))\n",
    "\n",
    "# Use the dedicated test set embeddings for recall calculation\n",
    "recall_test_queries = normalized_test_embeddings[:NUM_RECALL_QUERIES]\n",
    "\n",
    "print(f\"1. Running Brute-Force search on {NUM_RECALL_QUERIES} test queries...\")\n",
    "# .search_batched() is much faster for multiple queries\n",
    "true_neighbors, _ = bruteforce_searcher.search_batched(recall_test_queries)\n",
    "\n",
    "print(\"2. Running Optimized ScaNN search...\")\n",
    "scann_neighbors, _ = searcher.search_batched(recall_test_queries)\n",
    "\n",
    "# 5.3. Calculate and Print Recall\n",
    "recall_value = compute_recall(scann_neighbors, true_neighbors)\n",
    "print(f\"\\n‚úÖ Recall @{K_NEIGHBORS} for {NUM_RECALL_QUERIES} queries from the TEST split: {recall_value * 100:.2f}%\")\n",
    "print(\"This value indicates the percentage of exact nearest neighbors found by the approximate searcher.\")\n",
    "\n",
    "\n",
    "# --- 6. Running a Sample Query ---\n",
    "\n",
    "def run_query(query, search_index, original_dataset):\n",
    "    \"\"\"Embeds a query, normalizes it, and searches the ScaNN index.\"\"\"\n",
    "    print(f\"\\nSearching with query: '{query}'\")\n",
    "\n",
    "    # 6.1 Embed and Normalize the query\n",
    "    query_embedding = embedding_model.encode([query])[0]\n",
    "    normalized_query = query_embedding / np.linalg.norm(query_embedding)\n",
    "\n",
    "    # 6.2 Perform the search\n",
    "    # The 'k' parameter is configured during the builder step, so we omit it here.\n",
    "    indices, distances = search_index.search(normalized_query)\n",
    "\n",
    "    print(f\"\\nTop {len(indices)} results found:\")\n",
    "    for rank, (idx, distance) in enumerate(zip(indices, distances)):\n",
    "        print(f\"  Rank {rank+1}:\")\n",
    "        print(idx)\n",
    "        print(f\"    Text: {original_dataset[idx.item() ]}\")\n",
    "        # Dot product distance is 1.0 for perfect match, 0.0 for orthogonal\n",
    "        print(f\"    Similarity (Dot Product): {distance:.4f}\")\n",
    "        print(f\"    Dataset Index: {idx}\")\n",
    "\n",
    "# Run Query 1: Find sentences about computers\n",
    "run_query(query_text_1, searcher, dataset)\n",
    "\n",
    "# Run Query 2: Find sentences about weather/time\n",
    "run_query(query_text_2, searcher, dataset)\n",
    "\n",
    "# Run Query 3: Find relevant news articles\n",
    "run_query(query_text_3, searcher, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5djiaS-qPjp"
   },
   "source": [
    "# Benchmarking Section\n",
    "\n",
    "This second section attempts to run both the built-in brute force algorithm of ScaNN and the actual algorithm in a larger scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_noI77dxOHEK"
   },
   "outputs": [],
   "source": [
    "# -------------- development script used for generating an embedded! -----------\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Installation (Run this in a separate Colab cell first!) ---\n",
    "# Note: You now need 'datasets' installed.\n",
    "# !pip install scann sentence-transformers datasets\n",
    "\n",
    "try:\n",
    "    import scann\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from datasets import load_dataset # New import for public dataset\n",
    "except ImportError:\n",
    "    print(\"----------------------------------------------------------------------\")\n",
    "    print(\"üö® ERROR: Please run the following command in a separate Colab cell \")\n",
    "    print(\"and restart the runtime before running this code:\")\n",
    "    print(\"!pip install scann sentence-transformers datasets\")\n",
    "    print(\"----------------------------------------------------------------------\")\n",
    "    exit()\n",
    "\n",
    "print(\"This is a Colab script to load the dataset, embed, then save it into agnews_embeddings.h5.\")\n",
    "\n",
    "num_headlines = int(input(\"Enter the number of news headlines to convert, normalize, and be saved to the embeddings file.\"))\n",
    "\n",
    "if (num_headlines > 120000) or (num_headlines < 1):\n",
    "  print('Invalid input. num_headlines is set back to 5000.')\n",
    "  num_headlines = 5000\n",
    "\n",
    "# 2.\n",
    "\n",
    "MODEL_NAME = 'all-MiniLM-L6-v2'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "embedding_model = SentenceTransformer(MODEL_NAME, device=device)\n",
    "\n",
    "print(\"Loading public dataset (ag_news) subset...\")\n",
    "try:\n",
    "    ag_news_dataset_train = load_dataset('ag_news', split=f'train[:{num_headlines}]') # Loads dataset with num_training entries\n",
    "    dataset = ag_news_dataset_train['text']\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ag_news dataset. {e}. Since this is only used for benchmarking, the hardcoded dataset is ignored and the program is cancelled.\")\n",
    "\n",
    "#------- Inner function, declared right before use -------\n",
    "def generate_and_normalize(data):\n",
    "    \"\"\"Generates embeddings and performs L2 normalization.\"\"\"\n",
    "    print(f\"Generating embeddings for {len(data)} items...\")\n",
    "\n",
    "    embeddings = embedding_model.encode(\n",
    "        data,\n",
    "        convert_to_tensor=False,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    print(\"Normalizing embeddings...\")\n",
    "    normalized_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "    return normalized_embeddings, embeddings.shape[1]\n",
    "#------- Inner function, declared right before use -------\n",
    "\n",
    "normalized_dataset_embeddings, embedding_dim = generate_and_normalize(dataset)\n",
    "\n",
    "print(f\"\\nDataset generated: {normalized_dataset_embeddings.shape}\")\n",
    "print(f\"First dataset entry (Index Training Data): {dataset[0]}\")\n",
    "\n",
    "# This script has finished generating and normalizing. The next cell saves them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYLs5nN66rUt"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "filename = 'agnews_embeddings.h5'\n",
    "dataset_name = 'agnews'\n",
    "\n",
    "# 2. Save the embeddings to the H5 file\n",
    "try:\n",
    "    with h5py.File(filename, 'w') as f:\n",
    "        # Create a dataset to hold the embedding array\n",
    "        dset = f.create_dataset(dataset_name, data=normalized_dataset_embeddings)\n",
    "\n",
    "        # Optionally, you can add metadata as attributes\n",
    "        dset.attrs['description'] = 'Embeddings for my project'\n",
    "        dset.attrs['dimension'] = normalized_dataset_embeddings.shape[1]\n",
    "\n",
    "    print(f\"Embeddings successfully saved to {filename} under the dataset '{dataset_name}'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRt0vXiZh3Pv"
   },
   "source": [
    "# Benchmark section\n",
    "This used a pregen'd embedded dataset from a file (likely generated from the earlier snippets). Run from this point onwards to see time results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting h5py\n",
      "  Downloading h5py-3.15.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in ./scann-env/lib/python3.12/site-packages (from h5py) (2.3.4)\n",
      "Downloading h5py-3.15.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m eta \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: h5py\n",
      "Successfully installed h5py-3.15.1\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Z4wU2bkSMu3Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embeddings loaded successfully.\n",
      "Shape: (120000, 384)\n",
      "Metadata description: Embeddings for my project\n"
     ]
    }
   ],
   "source": [
    "# To load the embeddings back\n",
    "import h5py\n",
    "\n",
    "filename = 'agnews_embeddings.h5'\n",
    "dataset_name = 'agnews'\n",
    "\n",
    "try:\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        # Access the dataset\n",
    "        loaded_embeddings = f[dataset_name][:]\n",
    "\n",
    "        print(\"\\nEmbeddings loaded successfully.\")\n",
    "        print(\"Shape:\", loaded_embeddings.shape)\n",
    "        print(\"Metadata description:\", f[dataset_name].attrs['description'])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during loading: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fGYRopi-uMv5"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter k (the number of nearest neighbors to find):  10\n",
      "Enter the number of reorder candidates (recommended: 10*k):  500\n",
      "Enter the number of test queries to generate:  20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading public dataset (ag_news) subset...\n",
      "Generating embeddings for 20 items...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing embeddings...\n",
      "\n",
      "--- Building ScaNN Index with Dynamic Parameters ---\n",
      "Dataset size: 120000\n",
      "Number of leaves (clusters): 346\n",
      "Leaves to search: 34\n",
      "Training sample size: 82800\n",
      "K neighbors: 10\n",
      "Reorder candidates: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1762238254.415163   13532 partitioner_factory_base.cc:58] Size of sampled dataset for training partition: 82800\n",
      "I0000 00:00:1762238255.120589   13532 kmeans_tree_partitioner_utils.h:90] PartitionerFactory ran in 705.352746ms.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaNN optimized index built successfully.\n",
      "\n",
      "--- Computing Recall (ScaNN vs. Brute Force) ---\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter number of test queries for recall evaluation (or 'all' for complete test):  all\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on ALL 20 test queries (most accurate, takes longer)\n",
      "Running Optimized ScaNN search...\n",
      "\n",
      "Recall @10: 92.50%\n",
      "(Percentage of exact nearest neighbors found by ScaNN)\n",
      "Done. Brute-force time: 0.037970810997649096\n",
      "ScaNN time: 0.0024119370063999668\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "normalized_dataset_embeddings = loaded_embeddings\n",
    "num_headlines = loaded_embeddings.shape[0]\n",
    "\n",
    "K_NEIGHBORS = input(\"\\nEnter k (the number of nearest neighbors to find): \")\n",
    "try:\n",
    "    K_NEIGHBORS = int(K_NEIGHBORS)\n",
    "    if K_NEIGHBORS < 1 or K_NEIGHBORS > 100:\n",
    "        print(\"Invalid k. Setting to default: 5\")\n",
    "        K_NEIGHBORS = 5\n",
    "except ValueError:\n",
    "    print(\"Invalid k. Setting to default: 5\")\n",
    "    K_NEIGHBORS = 5\n",
    "\n",
    "REORDER_NEIGHBORS = input(\"Enter the number of reorder candidates (recommended: 10*k): \")\n",
    "try:\n",
    "    REORDER_NEIGHBORS = int(REORDER_NEIGHBORS)\n",
    "    if REORDER_NEIGHBORS < K_NEIGHBORS:\n",
    "        print(f\"Reorder candidates must be >= k. Setting to {K_NEIGHBORS * 10}\")\n",
    "        REORDER_NEIGHBORS = K_NEIGHBORS * 10\n",
    "except ValueError:\n",
    "    print(f\"Invalid input. Setting to {K_NEIGHBORS * 10}\")\n",
    "    REORDER_NEIGHBORS = K_NEIGHBORS * 10\n",
    "\n",
    "# ------ Section: Normalized TEST Embeddings ----\n",
    "\n",
    "num_tests = int(input(\"Enter the number of test queries to generate: \"))\n",
    "if (num_tests > 1000) or (num_tests < 1):\n",
    "  print('Invalid input. num_tests is set back to 100.')\n",
    "  num_tests = 100\n",
    "\n",
    "print(\"Loading public dataset (ag_news) subset...\")\n",
    "\n",
    "try:\n",
    "    # Load the test split (used for generating test queries for recall calculation)\n",
    "    ag_news_dataset_test = load_dataset('ag_news', split=f'test[:{num_tests}]')\n",
    "    test_dataset_text = ag_news_dataset_test['text']\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ag_news dataset: {e}\")\n",
    "\n",
    "#------- Inner function, declared right before use -------\n",
    "def generate_and_normalize(data):\n",
    "    \"\"\"Generates embeddings and performs L2 normalization.\"\"\"\n",
    "    print(f\"Generating embeddings for {len(data)} items...\")\n",
    "\n",
    "    embeddings = embedding_model.encode(\n",
    "        data,\n",
    "        convert_to_tensor=False,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    print(\"Normalizing embeddings...\")\n",
    "    normalized_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "    return normalized_embeddings, embeddings.shape[1]\n",
    "#------- Inner function, declared right before use -------\n",
    "\n",
    "normalized_test_embeddings, _ = generate_and_normalize(test_dataset_text)\n",
    "\n",
    "# --- 5. Dynamic ScaNN Parameters Based on Dataset Size ---\n",
    "print(\"\\n--- Building ScaNN Index with Dynamic Parameters ---\")\n",
    "\n",
    "# Calculate optimal parameters based on dataset size\n",
    "num_leaves = max(int(np.sqrt(num_headlines)), 100)  # rcm val: sqrt(num_hl);\n",
    "num_leaves_to_search = max(int(num_leaves * 0.1), 10)  # can't know for sure, requires tuning, will add prompt to enter this number later\n",
    "training_sample_size = min(int(num_headlines * 0.69), num_headlines - 1)  # 80% of dataset, prevents overfitting, fits for smaller dataset\n",
    "\n",
    "print(f\"Dataset size: {num_headlines}\")\n",
    "print(f\"Number of leaves (clusters): {num_leaves}\")\n",
    "print(f\"Leaves to search: {num_leaves_to_search}\")\n",
    "print(f\"Training sample size: {training_sample_size}\")\n",
    "print(f\"K neighbors: {K_NEIGHBORS}\")\n",
    "print(f\"Reorder candidates: {REORDER_NEIGHBORS}\")\n",
    "\n",
    "# --- 6. Build ScaNN Index ---\n",
    "builder = scann.scann_ops_pybind.builder(\n",
    "    normalized_dataset_embeddings,\n",
    "    K_NEIGHBORS,\n",
    "    \"dot_product\"\n",
    ")\n",
    "\n",
    "tree_configured = builder.tree(\n",
    "    num_leaves=num_leaves,\n",
    "    num_leaves_to_search=num_leaves_to_search,\n",
    "    training_sample_size=training_sample_size\n",
    ")\n",
    "\n",
    "ah_configured = tree_configured.score_ah(\n",
    "    8,  # Number of dimensions per subvector\n",
    "    anisotropic_quantization_threshold=0.2\n",
    ")\n",
    "\n",
    "reorder_configured = ah_configured.reorder(REORDER_NEIGHBORS)\n",
    "searcher = reorder_configured.build()\n",
    "\n",
    "print(\"ScaNN optimized index built successfully.\")\n",
    "\n",
    "# -----------\n",
    "\n",
    "print(\"\\n--- Computing Recall (ScaNN vs. Brute Force) ---\")\n",
    "\n",
    "def compute_recall(neighbors, true_neighbors):\n",
    "    \"\"\"Computes recall @k.\"\"\"\n",
    "    total = 0\n",
    "    for gt_row, row in zip(true_neighbors, neighbors):\n",
    "        total += np.intersect1d(gt_row, row).shape[0]\n",
    "    return total / true_neighbors.size\n",
    "\n",
    "# Build brute-force searcher\n",
    "bruteforce_searcher = scann.scann_ops_pybind.builder(\n",
    "    normalized_dataset_embeddings,\n",
    "    K_NEIGHBORS,\n",
    "    \"dot_product\"\n",
    ").score_brute_force().build()\n",
    "\n",
    "test_query_input = input(\"\\nEnter number of test queries for recall evaluation (or 'all' for complete test): \")\n",
    "\n",
    "if test_query_input.lower() == 'all':\n",
    "    NUM_RECALL_QUERIES = len(normalized_test_embeddings)\n",
    "    print(f\"Testing on ALL {NUM_RECALL_QUERIES} test queries (most accurate, takes longer)\")\n",
    "else:\n",
    "    try:\n",
    "        requested_queries = int(test_query_input)\n",
    "        NUM_RECALL_QUERIES = min(requested_queries, len(normalized_test_embeddings))\n",
    "        print(f\"Testing on {NUM_RECALL_QUERIES} test queries\")\n",
    "    except ValueError:\n",
    "        NUM_RECALL_QUERIES = min(1000, len(normalized_test_embeddings))\n",
    "        print(f\"Invalid input. Using default: {NUM_RECALL_QUERIES} test queries\")\n",
    "\n",
    "recall_test_queries = normalized_test_embeddings[:NUM_RECALL_QUERIES]\n",
    "\n",
    "brute_force_time_start = time.perf_counter()\n",
    "\n",
    "true_neighbors, _ = bruteforce_searcher.search_batched(recall_test_queries) # Brute-force searches\n",
    "\n",
    "brute_force_time_end = time.perf_counter()\n",
    "\n",
    "scann_time_start = time.perf_counter()\n",
    "\n",
    "print(\"Running Optimized ScaNN search...\")\n",
    "scann_neighbors, _ = searcher.search_batched(recall_test_queries)\n",
    "\n",
    "scann_time_end = time.perf_counter()\n",
    "\n",
    "recall_value = compute_recall(scann_neighbors, true_neighbors)\n",
    "print(f\"\\nRecall @{K_NEIGHBORS}: {recall_value * 100:.2f}%\")\n",
    "print(\"(Percentage of exact nearest neighbors found by ScaNN)\")\n",
    "print(f\"Done. Brute-force time: {brute_force_time_end - brute_force_time_start}\")\n",
    "print(f\"ScaNN time: {scann_time_end - scann_time_start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------- After benchmarking ---------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Search an article... baseball\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Searching with query: 'baseball'\n",
      "\n",
      "Top 10 results found:\n",
      "  Rank 1:\n",
      "69481\n",
      "    Text: Baseball and its fans recover from 1994 strike Ten years after the World Series was canceled and fans left in droves, Major League Baseball will tell you it has never been healthier.\n",
      "    Similarity (Dot Product): 0.6311\n",
      "    Dataset Index: 69481\n",
      "  Rank 2:\n",
      "11745\n",
      "    Text: Baseball Today * Abraham Nunez, Royals, hit his second grand slam in two weeks to lead Kansas City over Seattle 7-3. * Aaron Harang, Reds, limited St.\n",
      "    Similarity (Dot Product): 0.5888\n",
      "    Dataset Index: 11745\n",
      "  Rank 3:\n",
      "65440\n",
      "    Text: Red Sox-Yanks an instant classic Heroics and heartbreaks, diving catches and basepath blunders, hot bats and bizarre slumps, rainy days and endless nights. Already an instant classic, a short story transformed into a great American League \n",
      "    Similarity (Dot Product): 0.5738\n",
      "    Dataset Index: 65440\n",
      "  Rank 4:\n",
      "28122\n",
      "    Text: BBO Baseball Today (AP) AP - Texas at Oakland (10:05 p.m. EDT). Mark Mulder (17-4) and the A's face the Rangers.\n",
      "    Similarity (Dot Product): 0.5714\n",
      "    Dataset Index: 28122\n",
      "  Rank 5:\n",
      "48552\n",
      "    Text: What's That? Baseball's Back? I hadn't paid much attention to baseball lately -- what with the scintillating presidential campaign, the scintillating fall television season and Britney Spears's scintillating, latest marriage -- so imagine my surprise when I woke up the other afternoon to the latest developments:\n",
      "    Similarity (Dot Product): 0.5663\n",
      "    Dataset Index: 48552\n",
      "  Rank 6:\n",
      "67188\n",
      "    Text: Baseball Today (AP) AP - Houston at St. Louis (8:19 p.m. EDT). Game 7 of the NL championship series.\n",
      "    Similarity (Dot Product): 0.5632\n",
      "    Dataset Index: 67188\n",
      "  Rank 7:\n",
      "12730\n",
      "    Text: Let baseball solve its own problems Ever vigilant for new frontiers of American life where Congress can dictate the rules of behavior, elected officials in the nation #39;s capital have cast their eye on Major League Baseball.\n",
      "    Similarity (Dot Product): 0.5615\n",
      "    Dataset Index: 12730\n",
      "  Rank 8:\n",
      "86649\n",
      "    Text: Baseball #39;s image getting Creamed it #39;s the sport that won #39;t die, no matter what. You can have gamblers, psychotic fans, spouses that blab and players who cork.\n",
      "    Similarity (Dot Product): 0.5587\n",
      "    Dataset Index: 86649\n",
      "  Rank 9:\n",
      "18809\n",
      "    Text: Slide of the Yankees Passed up a chance to chat baseball with a couple of Yankees fans over a brew in a Pacific Beach pub in order to watch a softball game.\n",
      "    Similarity (Dot Product): 0.5564\n",
      "    Dataset Index: 18809\n",
      "  Rank 10:\n",
      "16678\n",
      "    Text: Baseball: Long, long night #39;s journey for Yanks NEW YORK The Yankees could not help but watch the scoreboard. Their heads swiveled repeatedly toward the outfield as hit after hit for Cleveland sailed over or against the wall.\n",
      "    Similarity (Dot Product): 0.5561\n",
      "    Dataset Index: 16678\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n----------- After benchmarking ---------\")\n",
    "\n",
    "manual_query = input(\"Search an article...\")\n",
    "\n",
    "# Pass the manual_query as a list to the generate_and_normalize function\n",
    "# normalized_query = generate_and_normalize([manual_query]) # This function is not meant to return the normalized query directly, it's better to use run_query\n",
    "\n",
    "def run_query(query, search_index, original_dataset):\n",
    "    \"\"\"Embeds a query, normalizes it, and searches the ScaNN index.\"\"\"\n",
    "    print(f\"\\nSearching with query: '{query}'\")\n",
    "\n",
    "    # 6.1 Embed and Normalize the query\n",
    "    query_embedding = embedding_model.encode([query])[0]\n",
    "    normalized_query = query_embedding / np.linalg.norm(query_embedding)\n",
    "\n",
    "    # 6.2 Perform the search\n",
    "    # The 'k' parameter is configured during the builder step, so we omit it here.\n",
    "    indices, distances = search_index.search(normalized_query)\n",
    "    print(f\"\\nTop {len(indices)} results found:\")\n",
    "    for rank, (idx, distance) in enumerate(zip(indices, distances)):\n",
    "        print(f\"  Rank {rank+1}:\")\n",
    "        print(idx)\n",
    "        print(f\"    Text: {original_dataset[idx.item() ]}\")\n",
    "        # Dot product distance is 1.0 for perfect match, 0.0 for orthogonal\n",
    "        print(f\"    Similarity (Dot Product): {distance:.4f}\")\n",
    "        print(f\"    Dataset Index: {idx}\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "# Load the full training dataset to match the loaded_embeddings size\n",
    "ag_news_dataset_full = load_dataset('ag_news', split='train')\n",
    "full_dataset_text = ag_news_dataset_full['text']\n",
    "\n",
    "run_query(manual_query, searcher, full_dataset_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
