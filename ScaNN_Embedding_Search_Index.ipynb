{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nackerboss/SCANN/blob/main/ScaNN_Embedding_Search_Index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP66bhdCtX6x"
      },
      "source": [
        "# A. Program Initialization\n",
        "\n",
        "This section installs the libraries, declares and implements the functions needed to run all of the following cells.\n",
        "\n",
        "1. Installs the relevant libraries.\n",
        "2. Removes TensorFlow\n",
        "3. Imports libraries and implements the common functions.\n",
        "\n",
        "To start, run all the cells in this section. The later sections' cells is not required to be run in an order and are for demos, benchmarking and searching.\n",
        "\n",
        "Take note; you should use a GPU/TPU runtime type!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w08aKpa29OBM"
      },
      "outputs": [],
      "source": [
        "!pip install scann sentence-transformers datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrNu6kxo_qwi"
      },
      "outputs": [],
      "source": [
        "!pip uninstall --yes tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sR0vDrfHxZJf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    import scann\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    from datasets import load_dataset # New import for public dataset\n",
        "except ImportError:\n",
        "    print(\"----------------------------------------------------------------------\")\n",
        "    print(\"ðŸš¨ ERROR: Please run the following command in a separate Colab cell \")\n",
        "    print(\"and rerun this cell before running this code:\")\n",
        "    print(\"!pip install scann sentence-transformers datasets\")\n",
        "    print(\"----------------------------------------------------------------------\")\n",
        "    exit()\n",
        "\n",
        "# --------------- Model used and device used (cpu/gpu/tpu) ----------------------\n",
        "\n",
        "MODEL_NAME = 'all-MiniLM-L6-v2'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "embedding_model = SentenceTransformer(MODEL_NAME, device=device)\n",
        "\n",
        "# --------------- Vector converter and normalization ----------------------------\n",
        "\n",
        "def generate_and_normalize(data):\n",
        "    \"\"\"Generates embeddings and performs L2 normalization.\"\"\"\n",
        "    print(f\"Generating embeddings for {len(data)} items...\")\n",
        "\n",
        "    # Generate embeddings (returns a numpy array)\n",
        "    embeddings = embedding_model.encode(\n",
        "        data,\n",
        "        convert_to_tensor=False,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    # L2 Normalization (Needed for ScaNN dot product or angular similarity)\n",
        "\n",
        "    print(\"Normalizing embeddings...\")\n",
        "    normalized_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "\n",
        "    return normalized_embeddings, embeddings.shape[1]\n",
        "\n",
        "# --------------------- Runs one text query -------------------------------------\n",
        "\n",
        "def run_query(query, search_index, original_dataset):\n",
        "    \"\"\"Embeds a query, normalizes it, and searches the ScaNN index.\"\"\"\n",
        "    print(f\"\\nSearching with query: '{query}'\")\n",
        "\n",
        "    # 6.1 Embed and Normalize the query\n",
        "    query_embedding = embedding_model.encode([query])[0]\n",
        "    normalized_query = query_embedding / np.linalg.norm(query_embedding)\n",
        "\n",
        "    # 6.2 Perform the search\n",
        "    # The 'k' parameter is configured during the builder step, so we omit it here.\n",
        "    indices, distances = search_index.search(normalized_query)\n",
        "\n",
        "    print(f\"\\nTop {len(indices)} results found:\")\n",
        "    for rank, (idx, distance) in enumerate(zip(indices, distances)):\n",
        "        print(f\"  Rank {rank+1}:\")\n",
        "        print(idx)\n",
        "        print(f\"    Text: {original_dataset[idx.item() ]}\")\n",
        "        # Dot product distance is 1.0 for perfect match, 0.0 for orthogonal\n",
        "        print(f\"    Similarity (Dot Product): {distance:.4f}\")\n",
        "        print(f\"    Dataset Index: {idx}\")\n",
        "\n",
        "# --------------- Computes recall (correctness of the ScaNN result) -------------\n",
        "\n",
        "def compute_recall(neighbors, true_neighbors):\n",
        "    \"\"\"\n",
        "    Computes recall @k by comparing the results of the approximate search\n",
        "    (neighbors) against the exact search (true_neighbors).\n",
        "    \"\"\"\n",
        "    total = 0\n",
        "    # Iterate through query results, comparing the approximate set against the true set\n",
        "    for gt_row, row in zip(true_neighbors, neighbors):\n",
        "        # Count the number of common elements (true positives)\n",
        "        total += np.intersect1d(gt_row, row).shape[0]\n",
        "\n",
        "    # Recall is (True Positives) / (Total True Neighbors)\n",
        "    return total / true_neighbors.size\n",
        "\n",
        "print(\"Importing libraries and implementing functions successful.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQvxHQXh3muF"
      },
      "source": [
        "# B. Demonstration of ScaNN Embedding Search Index\n",
        "\n",
        "This script demonstrates the general workings of ScaNN. This allows querying in a 2,000-dataset of news headlines, using predetermined texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ph879PTv9NK-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load a public dataset (ag_news) and take a manageable subset for demonstration\n",
        "print(\"Loading public dataset (ag_news) subset...\")\n",
        "try:\n",
        "    # Load the training split (used for building the ScaNN index)\n",
        "    ag_news_dataset_train = load_dataset('ag_news', split='train[:2000]')\n",
        "    dataset = ag_news_dataset_train['text']\n",
        "\n",
        "    # Load the test split (used for generating test queries for recall calculation)\n",
        "    ag_news_dataset_test = load_dataset('ag_news', split='test[:300]')\n",
        "    test_dataset_text = ag_news_dataset_test['text']\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading ag_news dataset: {e}\")\n",
        "    # Fallback to the original small dataset if loading fails\n",
        "    dataset = [\n",
        "        \"The sun rises in the east every morning.\",\n",
        "        \"A computer uses a central processing unit for core tasks.\",\n",
        "        \"Cats and dogs are common household pets.\",\n",
        "        \"A feline companion enjoying a nap on the sofa.\",\n",
        "        \"The central processing unit is the brain of any modern machine.\",\n",
        "        \"Tomorrow's forecast predicts clear skies and warm weather.\"\n",
        "    ]\n",
        "    test_dataset_text = dataset # Use the same small data for queries if primary fails\n",
        "\n",
        "\n",
        "# The queries we will use to search the dataset\n",
        "query_text_1 = \"The main component of a PC is the CPU.\"\n",
        "query_text_2 = \"What is the weather like at dawn?\"\n",
        "query_text_3 = \"Football match results from the weekend.\"\n",
        "\n",
        "\n",
        "\n",
        "normalized_dataset_embeddings, embedding_dim = generate_and_normalize(dataset)\n",
        "\n",
        "normalized_test_embeddings, _ = generate_and_normalize(test_dataset_text)\n",
        "\n",
        "print(f\"\\nDataset Ready. Shape: {normalized_dataset_embeddings.shape}\")\n",
        "print(f\"Test Query Set Shape: {normalized_test_embeddings.shape}\")\n",
        "print(f\"First dataset entry (Index Training Data): {dataset[0]}\")\n",
        "\n",
        "\n",
        "# --- 4. Building the ScaNN Index (Optimized for 2000 vectors) ---\n",
        "\n",
        "print(\"\\n--- 4. Building ScaNN Optimized Searcher (Trained on 2000 examples) ---\")\n",
        "\n",
        "# The maximum number of neighbors to retrieve (top-k)\n",
        "K_NEIGHBORS = 5\n",
        "REORDER_NEIGHBORS = 100 # Reduced reorder candidates for speedier demo\n",
        "\n",
        "# 4.1. Initialize the ScaNN builder\n",
        "# Arguments: (dataset, k, distance_metric)\n",
        "builder = scann.scann_ops_pybind.builder(\n",
        "    normalized_dataset_embeddings,\n",
        "    K_NEIGHBORS,\n",
        "    \"dot_product\"\n",
        ")\n",
        "\n",
        "# 4.2. Configure the Tree (Partitioning) stage\n",
        "tree_configured = builder.tree(\n",
        "    num_leaves=500,\n",
        "    num_leaves_to_search=50,\n",
        "    training_sample_size=4000\n",
        ")\n",
        "\n",
        "# 4.3. Configure Asymmetric Hashing (AH) for scoring\n",
        "ah_configured = tree_configured.score_ah(\n",
        "    2, # Number of dimensions per subvector\n",
        "    anisotropic_quantization_threshold=0.2\n",
        ")\n",
        "\n",
        "# 4.4. Configure the Reordering (Refinement) stage\n",
        "reorder_configured = ah_configured.reorder(REORDER_NEIGHBORS)\n",
        "\n",
        "# 4.5. Finalize and build the searcher\n",
        "searcher = reorder_configured.build()\n",
        "\n",
        "print(\"ScaNN optimized index built successfully.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-IYIPbMCrdT"
      },
      "outputs": [],
      "source": [
        "# --- Computing Recall (ScaNN vs. Brute Force) ---\n",
        "\n",
        "print(\"\\n--- 5. Computing Recall (ScaNN vs. Brute Force) ---\")\n",
        "\n",
        "# Create a Brute-Force ScaNN searcher (no tree, no quantization)\n",
        "# This will find the mathematically exact nearest neighbors.\n",
        "bruteforce_searcher = scann.scann_ops_pybind.builder(\n",
        "    normalized_dataset_embeddings,\n",
        "    K_NEIGHBORS,\n",
        "    \"dot_product\"\n",
        ").score_brute_force().build()\n",
        "\n",
        "# 5.2. Define Test Queries (using a subset of the official test split as queries)\n",
        "# Limit the number of test queries for faster recall computation\n",
        "MAX_TEST_QUERIES = 500\n",
        "NUM_RECALL_QUERIES = min(MAX_TEST_QUERIES, len(normalized_test_embeddings))\n",
        "\n",
        "# Use the dedicated test set embeddings for recall calculation\n",
        "recall_test_queries = normalized_test_embeddings[:NUM_RECALL_QUERIES]\n",
        "\n",
        "print(f\"1. Running Brute-Force search on {NUM_RECALL_QUERIES} test queries...\")\n",
        "# .search_batched() is much faster for multiple queries\n",
        "true_neighbors, _ = bruteforce_searcher.search_batched(recall_test_queries)\n",
        "\n",
        "print(\"2. Running Optimized ScaNN search...\")\n",
        "scann_neighbors, _ = searcher.search_batched(recall_test_queries)\n",
        "\n",
        "# 5.3. Calculate and Print Recall\n",
        "recall_value = compute_recall(scann_neighbors, true_neighbors)\n",
        "print(f\"\\nâœ… Recall @{K_NEIGHBORS} for {NUM_RECALL_QUERIES} queries from the TEST split: {recall_value * 100:.2f}%\")\n",
        "print(\"This value indicates the percentage of exact nearest neighbors found by the approximate searcher.\")\n",
        "\n",
        "# Run Query 1: Find sentences about computers\n",
        "run_query(query_text_1, searcher, dataset)\n",
        "\n",
        "# Run Query 2: Find sentences about weather/time\n",
        "run_query(query_text_2, searcher, dataset)\n",
        "\n",
        "# Run Query 3: Find relevant news articles\n",
        "run_query(query_text_3, searcher, dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_arbitrary_query(query, search_index, original_dataset, k_neighbors):\n",
        "    \"\"\"Embeds a query, normalizes it, and searches the ScaNN index.\"\"\"\n",
        "    print(f\"\\nSearching with query: '{query}'\")\n",
        "\n",
        "    query_embedding = embedding_model.encode([query])[0]\n",
        "    normalized_query = query_embedding / np.linalg.norm(query_embedding)\n",
        "\n",
        "    indices, distances = search_index.search(normalized_query, leaves_to_search = k_neighbors * 20, final_num_neighbors = k_neighbors, pre_reorder_num_neighbors = k_neighbors * 5)\n",
        "\n",
        "    print(f\"\\nTop {len(indices)} results found:\")\n",
        "    for rank, (idx, distance) in enumerate(zip(indices, distances)):\n",
        "        print(f\"  Rank {rank+1}:\")\n",
        "        print(idx)\n",
        "        print(f\"    Text: {original_dataset[idx.item() ]}\")\n",
        "        # Dot product distance is 1.0 for perfect match, 0.0 for orthogonal\n",
        "        print(f\"    Similarity (Dot Product): {distance:.4f}\")\n",
        "        print(f\"    Dataset Index: {idx}\")\n",
        "\n",
        "print(\"-------------- Arbitrary Query ---------------\")\n",
        "\n",
        "while (True):\n",
        "  query_text = input(\"Enter a query... ('quit' to exit) \")\n",
        "\n",
        "  if (query_text == \"quit\"):\n",
        "    break\n",
        "\n",
        "  k = input(\"Enter how much neighbors is needed (0 to exit) \")\n",
        "\n",
        "  if (k == 0):\n",
        "    break\n",
        "\n",
        "  arbitrary_searcher = reorder_configured.build()\n",
        "\n",
        "  run_arbitrary_query(query_text, arbitrary_searcher, dataset, int(k))\n",
        "\n",
        "  print(\"\\n\\n-------------------------------------------\\n\\n\")"
      ],
      "metadata": {
        "id": "XcYpnOi_E39I"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}