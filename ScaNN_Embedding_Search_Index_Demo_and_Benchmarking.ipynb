{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nackerboss/SCANN/blob/main/ScaNN_Embedding_Search_Index_Demo_and_Benchmarking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP66bhdCtX6x"
      },
      "source": [
        "# A. Program Initialization\n",
        "\n",
        "This section installs the libraries, declares and implements the functions needed to run all of the following cells.\n",
        "\n",
        "1. Installs the relevant libraries.\n",
        "2. Removes TensorFlow\n",
        "3. Imports libraries and implements the common functions.\n",
        "\n",
        "To start, run all the cells in this section. The later sections' cells is not required to be run in an order and are for demos, benchmarking and searching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w08aKpa29OBM"
      },
      "outputs": [],
      "source": [
        "!pip install scann sentence-transformers datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrNu6kxo_qwi"
      },
      "outputs": [],
      "source": [
        "!pip uninstall --yes tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sR0vDrfHxZJf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    import scann\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    from datasets import load_dataset # New import for public dataset\n",
        "except ImportError:\n",
        "    print(\"----------------------------------------------------------------------\")\n",
        "    print(\"ðŸš¨ ERROR: Please run the following command in a separate Colab cell \")\n",
        "    print(\"and rerun this cell before running this code:\")\n",
        "    print(\"!pip install scann sentence-transformers datasets\")\n",
        "    print(\"----------------------------------------------------------------------\")\n",
        "    exit()\n",
        "\n",
        "# --------------- Model used and device used (cpu/gpu/tpu) ----------------------\n",
        "\n",
        "MODEL_NAME = 'all-MiniLM-L6-v2'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "embedding_model = SentenceTransformer(MODEL_NAME, device=device)\n",
        "\n",
        "# --------------- Vector converter and normalization ----------------------------\n",
        "\n",
        "def generate_and_normalize(data):\n",
        "    \"\"\"Generates embeddings and performs L2 normalization.\"\"\"\n",
        "    print(f\"Generating embeddings for {len(data)} items...\")\n",
        "\n",
        "    # Generate embeddings (returns a numpy array)\n",
        "    embeddings = embedding_model.encode(\n",
        "        data,\n",
        "        convert_to_tensor=False,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    # L2 Normalization (Needed for ScaNN dot product or angular similarity)\n",
        "\n",
        "    print(\"Normalizing embeddings...\")\n",
        "    normalized_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "\n",
        "    return normalized_embeddings, embeddings.shape[1]\n",
        "\n",
        "# --------------------- Runs one text query -------------------------------------\n",
        "\n",
        "def run_query(query, search_index, original_dataset):\n",
        "    \"\"\"Embeds a query, normalizes it, and searches the ScaNN index.\"\"\"\n",
        "    print(f\"\\nSearching with query: '{query}'\")\n",
        "\n",
        "    # 6.1 Embed and Normalize the query\n",
        "    query_embedding = embedding_model.encode([query])[0]\n",
        "    normalized_query = query_embedding / np.linalg.norm(query_embedding)\n",
        "\n",
        "    # 6.2 Perform the search\n",
        "    # The 'k' parameter is configured during the builder step, so we omit it here.\n",
        "    indices, distances = search_index.search(normalized_query)\n",
        "\n",
        "    print(f\"\\nTop {len(indices)} results found:\")\n",
        "    for rank, (idx, distance) in enumerate(zip(indices, distances)):\n",
        "        print(f\"  Rank {rank+1}:\")\n",
        "        print(idx)\n",
        "        print(f\"    Text: {original_dataset[idx.item() ]}\")\n",
        "        # Dot product distance is 1.0 for perfect match, 0.0 for orthogonal\n",
        "        print(f\"    Similarity (Dot Product): {distance:.4f}\")\n",
        "        print(f\"    Dataset Index: {idx}\")\n",
        "\n",
        "# --------------- Computes recall (correctness of the ScaNN result) -------------\n",
        "\n",
        "def compute_recall(neighbors, true_neighbors):\n",
        "    \"\"\"\n",
        "    Computes recall @k by comparing the results of the approximate search\n",
        "    (neighbors) against the exact search (true_neighbors).\n",
        "    \"\"\"\n",
        "    total = 0\n",
        "    # Iterate through query results, comparing the approximate set against the true set\n",
        "    for gt_row, row in zip(true_neighbors, neighbors):\n",
        "        # Count the number of common elements (true positives)\n",
        "        total += np.intersect1d(gt_row, row).shape[0]\n",
        "\n",
        "    # Recall is (True Positives) / (Total True Neighbors)\n",
        "    return total / true_neighbors.size\n",
        "\n",
        "print(\"Importing libraries and implementing functions successful.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQvxHQXh3muF"
      },
      "source": [
        "# B. Demonstration of ScaNN Embedding Search Index\n",
        "\n",
        "This script demonstrates the general workings of ScaNN. This allows querying in a 2,000-dataset of news headlines, using predetermined texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ph879PTv9NK-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load a public dataset (ag_news) and take a manageable subset for demonstration\n",
        "print(\"Loading public dataset (ag_news) subset...\")\n",
        "try:\n",
        "    # Load the training split (used for building the ScaNN index)\n",
        "    ag_news_dataset_train = load_dataset('ag_news', split='train[:2000]')\n",
        "    dataset = ag_news_dataset_train['text']\n",
        "\n",
        "    # Load the test split (used for generating test queries for recall calculation)\n",
        "    ag_news_dataset_test = load_dataset('ag_news', split='test[:300]')\n",
        "    test_dataset_text = ag_news_dataset_test['text']\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading ag_news dataset: {e}\")\n",
        "    # Fallback to the original small dataset if loading fails\n",
        "    dataset = [\n",
        "        \"The sun rises in the east every morning.\",\n",
        "        \"A computer uses a central processing unit for core tasks.\",\n",
        "        \"Cats and dogs are common household pets.\",\n",
        "        \"A feline companion enjoying a nap on the sofa.\",\n",
        "        \"The central processing unit is the brain of any modern machine.\",\n",
        "        \"Tomorrow's forecast predicts clear skies and warm weather.\"\n",
        "    ]\n",
        "    test_dataset_text = dataset # Use the same small data for queries if primary fails\n",
        "\n",
        "\n",
        "# The queries we will use to search the dataset\n",
        "query_text_1 = \"The main component of a PC is the CPU.\"\n",
        "query_text_2 = \"What is the weather like at dawn?\"\n",
        "query_text_3 = \"Football match results from the weekend.\"\n",
        "\n",
        "\n",
        "\n",
        "normalized_dataset_embeddings, embedding_dim = generate_and_normalize(dataset)\n",
        "\n",
        "normalized_test_embeddings, _ = generate_and_normalize(test_dataset_text)\n",
        "\n",
        "print(f\"\\nDataset Ready. Shape: {normalized_dataset_embeddings.shape}\")\n",
        "print(f\"Test Query Set Shape: {normalized_test_embeddings.shape}\")\n",
        "print(f\"First dataset entry (Index Training Data): {dataset[0]}\")\n",
        "\n",
        "\n",
        "# --- 4. Building the ScaNN Index (Optimized for 2000 vectors) ---\n",
        "\n",
        "print(\"\\n--- 4. Building ScaNN Optimized Searcher (Trained on 2000 examples) ---\")\n",
        "\n",
        "# The maximum number of neighbors to retrieve (top-k)\n",
        "K_NEIGHBORS = 5\n",
        "REORDER_NEIGHBORS = 100 # Reduced reorder candidates for speedier demo\n",
        "\n",
        "# 4.1. Initialize the ScaNN builder\n",
        "# Arguments: (dataset, k, distance_metric)\n",
        "builder = scann.scann_ops_pybind.builder(\n",
        "    normalized_dataset_embeddings,\n",
        "    K_NEIGHBORS,\n",
        "    \"dot_product\"\n",
        ")\n",
        "\n",
        "# 4.2. Configure the Tree (Partitioning) stage\n",
        "tree_configured = builder.tree(\n",
        "    num_leaves=500,\n",
        "    num_leaves_to_search=50,\n",
        "    training_sample_size=4000\n",
        ")\n",
        "\n",
        "# 4.3. Configure Asymmetric Hashing (AH) for scoring\n",
        "ah_configured = tree_configured.score_ah(\n",
        "    2, # Number of dimensions per subvector\n",
        "    anisotropic_quantization_threshold=0.2\n",
        ")\n",
        "\n",
        "# 4.4. Configure the Reordering (Refinement) stage\n",
        "reorder_configured = ah_configured.reorder(REORDER_NEIGHBORS)\n",
        "\n",
        "# 4.5. Finalize and build the searcher\n",
        "searcher = reorder_configured.build()\n",
        "\n",
        "print(\"ScaNN optimized index built successfully.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-IYIPbMCrdT"
      },
      "outputs": [],
      "source": [
        "# --- Computing Recall (ScaNN vs. Brute Force) ---\n",
        "\n",
        "print(\"\\n--- 5. Computing Recall (ScaNN vs. Brute Force) ---\")\n",
        "\n",
        "# Create a Brute-Force ScaNN searcher (no tree, no quantization)\n",
        "# This will find the mathematically exact nearest neighbors.\n",
        "bruteforce_searcher = scann.scann_ops_pybind.builder(\n",
        "    normalized_dataset_embeddings,\n",
        "    K_NEIGHBORS,\n",
        "    \"dot_product\"\n",
        ").score_brute_force().build()\n",
        "\n",
        "# 5.2. Define Test Queries (using a subset of the official test split as queries)\n",
        "# Limit the number of test queries for faster recall computation\n",
        "MAX_TEST_QUERIES = 500\n",
        "NUM_RECALL_QUERIES = min(MAX_TEST_QUERIES, len(normalized_test_embeddings))\n",
        "\n",
        "# Use the dedicated test set embeddings for recall calculation\n",
        "recall_test_queries = normalized_test_embeddings[:NUM_RECALL_QUERIES]\n",
        "\n",
        "print(f\"1. Running Brute-Force search on {NUM_RECALL_QUERIES} test queries...\")\n",
        "# .search_batched() is much faster for multiple queries\n",
        "true_neighbors, _ = bruteforce_searcher.search_batched(recall_test_queries)\n",
        "\n",
        "print(\"2. Running Optimized ScaNN search...\")\n",
        "scann_neighbors, _ = searcher.search_batched(recall_test_queries)\n",
        "\n",
        "# 5.3. Calculate and Print Recall\n",
        "recall_value = compute_recall(scann_neighbors, true_neighbors)\n",
        "print(f\"\\nâœ… Recall @{K_NEIGHBORS} for {NUM_RECALL_QUERIES} queries from the TEST split: {recall_value * 100:.2f}%\")\n",
        "print(\"This value indicates the percentage of exact nearest neighbors found by the approximate searcher.\")\n",
        "\n",
        "# Run Query 1: Find sentences about computers\n",
        "run_query(query_text_1, searcher, dataset)\n",
        "\n",
        "# Run Query 2: Find sentences about weather/time\n",
        "run_query(query_text_2, searcher, dataset)\n",
        "\n",
        "# Run Query 3: Find relevant news articles\n",
        "run_query(query_text_3, searcher, dataset)\n",
        "\n",
        "print(\"-------------- Arbitrary Query ---------------\")\n",
        "\n",
        "while (True):\n",
        "  query_text = input(\"Enter a query... ('quit' to exit) \")\n",
        "  k = input(\"Enter how much neighbors is needed (0 to exit) \")\n",
        "\n",
        "  if (query_text == \"quit\" or k == 0):\n",
        "    break\n",
        "\n",
        "  builder = scann.scann_ops_pybind.builder(\n",
        "    normalized_dataset_embeddings,\n",
        "    k,\n",
        "    \"dot_product\"\n",
        "  )\n",
        "\n",
        "  tree_configured = builder.tree(\n",
        "    num_leaves=500,\n",
        "    num_leaves_to_search=50,\n",
        "    training_sample_size=4000\n",
        "  )\n",
        "\n",
        "  ah_configured = tree_configured.score_ah(\n",
        "    2, # Number of dimensions per subvector\n",
        "    anisotropic_quantization_threshold=0.2\n",
        "  )\n",
        "\n",
        "  reorder_configured = ah_configured.reorder(REORDER_NEIGHBORS)\n",
        "\n",
        "  arbitrary_searcher = reorder_configured.build()\n",
        "\n",
        "  run_query(query_text, searcher, dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5djiaS-qPjp"
      },
      "source": [
        "# C. Benchmarking Section\n",
        "\n",
        "This second section attempts to run both the built-in brute force algorithm of ScaNN and the actual algorithm in a larger scale.\n",
        "\n",
        "The first cell is for pre-generating a subset (or all) of the headlines in ag_news for quicker use(that it does not need to regenerate the vectors again). If you want to use an external file (or a file in ann-benchmark, which we have a link to), skip the next two cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_noI77dxOHEK"
      },
      "outputs": [],
      "source": [
        "# -------------- development script used for generating an embedded! -----------\n",
        "\n",
        "print(\"This is a Colab script to load the dataset, embed, then save it into agnews_embeddings.h5.\")\n",
        "\n",
        "num_headlines = int(input(\"Enter the number of news headlines to convert, normalize, and be saved to the embeddings file. \"))\n",
        "\n",
        "if (num_headlines > 120000) or (num_headlines < 1):\n",
        "  print('Invalid input. num_headlines is set back to 5000.')\n",
        "  num_headlines = 5000\n",
        "\n",
        "num_tests = int(input(\"Enter the number of text queries to be converted, normalized, and be saved. \"))\n",
        "\n",
        "if (num_tests > 7600) or (num_tests < 1):\n",
        "  print('Invalid input. num_tests is set back to 1000.')\n",
        "  num_tests = 1000\n",
        "\n",
        "# 2.\n",
        "\n",
        "MODEL_NAME = 'all-MiniLM-L6-v2'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "embedding_model = SentenceTransformer(MODEL_NAME, device=device)\n",
        "\n",
        "print(\"Loading public dataset (ag_news) subset...\")\n",
        "try:\n",
        "    ag_news_dataset_train = load_dataset('ag_news', split=f'train[:{num_headlines}]') # Loads dataset with num_training entries\n",
        "    dataset_train = ag_news_dataset_train['text']\n",
        "\n",
        "    ag_news_dataset_test = load_dataset('ag_news', split=f'test[:{num_tests}]') # Loads dataset with num_test entries\n",
        "    dataset_test = ag_news_dataset_test['text']\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading ag_news dataset. {e}. Since this is used for benchmarking, the hardcoded dataset is ignored and the program is cancelled.\")\n",
        "    exit()\n",
        "\n",
        "normalized_dataset_train_embeddings, embedding_dim = generate_and_normalize(dataset_train)\n",
        "normalized_dataset_test_embeddings, embedding_dim = generate_and_normalize(dataset_test)\n",
        "\n",
        "print(f\"\\nDataset generated: {normalized_dataset_train_embeddings.shape}\")\n",
        "print(f\"First dataset training entry (Index Training Data): {dataset_train[0]}\")\n",
        "\n",
        "GT_K_NEIGHBORS = 100\n",
        "\n",
        "print(\"Generating ground truth neighbors via brute-force search...\")\n",
        "\n",
        "# Create a Brute-Force ScaNN searcher to find the mathematically exact nearest neighbors.\n",
        "bruteforce_searcher_gt = scann.scann_ops_pybind.builder(\n",
        "    normalized_dataset_train_embeddings,\n",
        "    GT_K_NEIGHBORS,\n",
        "    \"dot_product\"\n",
        ").score_brute_force().build()\n",
        "\n",
        "# Perform the brute-force search on the test embeddings to get true neighbors\n",
        "ground_truth_neighbors, _ = bruteforce_searcher_gt.search_batched(normalized_dataset_test_embeddings)\n",
        "\n",
        "print(f\"Ground truth neighbors generated. Shape: {ground_truth_neighbors.shape}\")\n",
        "\n",
        "# This script has finished generating and normalizing. The next cell saves them.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dbdd87d"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "\n",
        "filename = 'agnews_embeddings.h5'\n",
        "dataset_train = 'train'\n",
        "dataset_test = 'test'\n",
        "dataset_truth = 'neighbors'\n",
        "\n",
        "# 2. Save the embeddings to the H5 file\n",
        "try:\n",
        "    with h5py.File(filename, 'w') as f:\n",
        "        dtrset = f.create_dataset(dataset_train, data=normalized_dataset_train_embeddings)\n",
        "        dteset = f.create_dataset(dataset_test, data=normalized_dataset_test_embeddings)\n",
        "        dgtset = f.create_dataset(dataset_truth, data=ground_truth_neighbors) # Save ground truth\n",
        "\n",
        "    print(f\"Embeddings successfully saved to {filename} under the data train set '{dataset_train}', data test set '{dataset_test}', and ground truth neighbors '{dataset_truth}'.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRt0vXiZh3Pv"
      },
      "source": [
        "This used a pregen'd embedded dataset from a file. Run this snippet first to fetch the vectors into the variables.\n",
        "\n",
        "The first cell downloads an external test file for benchmarking. You can replace the link by any other link to get other .hdf5 files.\n",
        "\n",
        "If you wish to do this with any arbitrary .hdf5 (.h5) files, upload that file first, ignore the first cell, modify the 'filename' variable in the second cell. Default is GloVe-50, 50 dimensions, ~1.2m train size, 10k test size, available [here](https://github.com/erikbern/ann-benchmarks?tab=readme-ov-file). As the file has 'train', 'test' and 'neighbors' objects, we have hardcoded this into the cell. This remains the standard for the other datasets in the other files on the link mentioned.\n",
        "\n",
        "It is recommended that you use http://ann-benchmarks.com/glove-200-angular.hdf5 this data set for just brute-force and ScaNN comparison. ScaNN's tree builder is infeasable to be repeated a lot with this dataset to properly benchmark k-values, and thus, is not recommended for k-value benchmarking.\n",
        "\n",
        "Be warned: We are using ScaNN by dot products. Remember to use angular or dot product datasets of ground truths!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ev6Nxp1lOUAn"
      },
      "outputs": [],
      "source": [
        "!wget http://ann-benchmarks.com/glove-50-angular.hdf5"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember to edit the filename string value for the script to work!"
      ],
      "metadata": {
        "id": "JvIuHKDeN6to"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4wU2bkSMu3Z"
      },
      "outputs": [],
      "source": [
        "# To load the embeddings back\n",
        "import h5py\n",
        "\n",
        "filename = 'glove-50-angular.hdf5'\n",
        "dataset_train = 'train'\n",
        "dataset_test = 'test'\n",
        "dataset_truth = 'neighbors'\n",
        "\n",
        "try:\n",
        "    with h5py.File(filename, 'r') as f:\n",
        "        # Access the dataset, then normalizes the embedding vectors. Be warned, this can only work for Angular/Dot products datasets!\n",
        "        loaded_train_embeddings_raw = f[dataset_train][:]\n",
        "        loaded_train_embeddings = loaded_train_embeddings_raw / np.linalg.norm(loaded_train_embeddings_raw, axis=1, keepdims=True)\n",
        "        loaded_test_embeddings_raw = f[dataset_test][:]\n",
        "        loaded_test_embeddings = loaded_test_embeddings_raw / np.linalg.norm(loaded_test_embeddings_raw, axis=1, keepdims=True)\n",
        "        loaded_truth_embedding_raw = f[dataset_truth][:]\n",
        "        loaded_truth_embedding = loaded_truth_embedding_raw / np.linalg.norm(loaded_truth_embedding_raw, axis=1, keepdims=True)\n",
        "\n",
        "        print(\"\\nEmbeddings loaded successfully.\")\n",
        "        print(\"Train shape:\", loaded_train_embeddings.shape)\n",
        "        print(\"Test shape:\", loaded_test_embeddings.shape)\n",
        "        print(\"Truth shape:\", loaded_truth_embedding.shape)\n",
        "\n",
        " #       print(\"Metadata description:\", f[dataset_name].attrs['description'])\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during loading: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iUsT7x23fa0"
      },
      "source": [
        "This next cell starts running the relevant code to benchmark runtimes of the ScaNN algorithm and the built-in brute-force algorithm. The K_NEIGHBORS and TEST_SIZES are hardcoded, with TEXT_SIZES being an array for multiple test and allowing to see trends."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8VynpmvEafj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import tempfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "import scann\n",
        "import psutil\n",
        "\n",
        "K_NEIGHBORS = 10\n",
        "\n",
        "TEST_SIZES = [100, 300, 1000, 3000, 10000]\n",
        "\n",
        "def get_process_memory_mb():\n",
        "    \"\"\"Get current process memory usage in MB.\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    return process.memory_info().rss / (1024 * 1024)\n",
        "\n",
        "\n",
        "def sizeof(obj):\n",
        "    \"\"\"Accurate memory estimate (MB).\"\"\"\n",
        "    if isinstance(obj, np.ndarray):\n",
        "        return obj.nbytes / (1024 * 1024)\n",
        "    try:\n",
        "        return sys.getsizeof(obj) / (1024 * 1024)\n",
        "    except:\n",
        "        return -1\n",
        "\n",
        "\n",
        "\n",
        "def benchmark_instance_speed(num_queries, dataset_embeddings, searcher, test_embeddings):\n",
        "\n",
        "  print(f\"\\n=== Benchmarking with {num_queries} Queries ===\")\n",
        "\n",
        "  if num_queries > test_embeddings.shape[0]:\n",
        "    raise ValueError(f\"Not enough queries in the test embeddings! {num_queries} > {test_embeddings.shape[0]}\")\n",
        "\n",
        "  query_batch = test_embeddings[:num_queries]\n",
        "\n",
        "  brute = scann.scann_ops_pybind.builder(\n",
        "      dataset_embeddings,\n",
        "      K_NEIGHBORS,\n",
        "      \"dot_product\"\n",
        "  ).score_brute_force().build()\n",
        "\n",
        "  # ---- Brute force ----\n",
        "  bf_start = time.perf_counter()\n",
        "  true_neighbors, _ = brute.search_batched(query_batch)\n",
        "  bf_end = time.perf_counter()\n",
        "  bf_time = bf_end - bf_start\n",
        "\n",
        "  # ---- ScaNN ----\n",
        "  sc_start = time.perf_counter()\n",
        "  scann_neighbors, _ = searcher.search_batched(query_batch)\n",
        "  sc_end = time.perf_counter()\n",
        "  sc_time = sc_end - sc_start\n",
        "\n",
        "  # ---- Recall ----\n",
        "  recall_value = compute_recall(scann_neighbors, true_neighbors)\n",
        "\n",
        "  return {\n",
        "    \"num_queries\": num_queries,\n",
        "    \"recall@{}\".format(K_NEIGHBORS): recall_value,\n",
        "    \"brute_force_time_sec\": bf_time,\n",
        "    \"scann_time_sec\": sc_time,\n",
        "    \"speedup\": bf_time / sc_time if sc_time > 0 else np.inf\n",
        "  }\n",
        "\n",
        "def sub_benchmark_speed():\n",
        "\n",
        "\n",
        "  loaded_train_embeddings\n",
        "  num_headlines = loaded_train_embeddings.shape[0]\n",
        "\n",
        "  print(f\"\\nDataset size: {num_headlines}\")\n",
        "\n",
        "  print(\"Loaded dataset embeddings.\")\n",
        "  raw_embedding_size_mb = sizeof(loaded_train_embeddings)\n",
        "  print(f\"Raw embedding memory usage: {raw_embedding_size_mb:.2f} MB\")\n",
        "\n",
        "  print(f\"\\nDataset size: {num_headlines}\")\n",
        "\n",
        "  print(\"Loaded dataset embeddings.\")\n",
        "  raw_embedding_size_mb = sizeof(loaded_train_embeddings)\n",
        "  print(f\"Raw embedding memory usage: {raw_embedding_size_mb:.2f} MB\")\n",
        "\n",
        "  # ===================================\n",
        "  # Build ScaNN index\n",
        "  # ===================================\n",
        "  print(\"Building ScaNN index...\")\n",
        "\n",
        "  mem_before_index = get_process_memory_mb()\n",
        "\n",
        "  num_leaves = max(int(np.sqrt(num_headlines)), 100)  # rcm val: sqrt(num_hl);\n",
        "  num_leaves_to_search = max(int(num_leaves * 0.05), 10)  # requires tuning in order to get the desired recall value\n",
        "  training_sample_size = min(int(num_headlines * 0.3), num_headlines - 1)  # ~30% of dataset\n",
        "  reordered = K_NEIGHBORS * 10\n",
        "\n",
        "  searcher = scann.scann_ops_pybind.builder(\n",
        "      loaded_train_embeddings,\n",
        "      K_NEIGHBORS,\n",
        "      \"dot_product\"\n",
        "  ).tree(\n",
        "      num_leaves=num_leaves,\n",
        "      num_leaves_to_search=num_leaves_to_search,\n",
        "      training_sample_size=training_sample_size\n",
        "  ).score_ah(\n",
        "      dimensions_per_block=2,\n",
        "      anisotropic_quantization_threshold=0.2\n",
        "  ).reorder(K_NEIGHBORS * 30).build()\n",
        "\n",
        "  mem_after_index = get_process_memory_mb()\n",
        "  scann_index_memory_mb = mem_after_index - mem_before_index\n",
        "\n",
        "  print(\"ScaNN index built successfully.\")\n",
        "  print(f\"ScaNN index memory usage: {scann_index_memory_mb:.2f} MB\")\n",
        "  compression_ratio = raw_embedding_size_mb / scann_index_memory_mb if scann_index_memory_mb > 0 else 0\n",
        "  print(f\"Compression ratio: {compression_ratio:.3f}x\")\n",
        "\n",
        "  # ===================================\n",
        "  # Run benchmarks\n",
        "  # ===================================\n",
        "  results = []\n",
        "\n",
        "  for size in TEST_SIZES:\n",
        "      stats = benchmark_instance_speed(size, loaded_train_embeddings, searcher, loaded_test_embeddings)\n",
        "      results.append(stats)\n",
        "\n",
        "  # Convert to table\n",
        "  df = pd.DataFrame(results)\n",
        "\n",
        "  # print(\"\\n=== Index Statistics ===\")\n",
        "  # print(f\"Raw embedding memory usage: {raw_embedding_size_mb:.2f} MB\")\n",
        "  # print(f\"ScaNN index size: {scann_index_size_mb:.2f} MB\")\n",
        "  # print(f\"Compression ratio: {compression_ratio:.3f}x\")\n",
        "\n",
        "  print(\"\\n=== Final Benchmark Table (per query count only) ===\")\n",
        "  print(df)\n",
        "\n",
        "  df.to_csv(\"benchmark_results.csv\", index=False)\n",
        "  print(\"Saved CSV -> benchmark_results.csv\")\n",
        "\n",
        "  # ===================================\n",
        "  # Plotting\n",
        "  # ===================================\n",
        "\n",
        "  # ---- Speedup Plot ----\n",
        "  plt.figure()\n",
        "  plt.plot(df[\"num_queries\"], df[\"brute_force_time_sec\"], label=\"Brute-force\", marker='o')\n",
        "  plt.plot(df[\"num_queries\"], df[\"scann_time_sec\"], label=\"ScaNN\", marker='s')\n",
        "  plt.xlabel(\"num. queries\")\n",
        "  plt.ylabel(\"Time (seconds)\")\n",
        "  plt.title(\"Searching time comparison\")\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.savefig(\"time_comparison_plot.png\")\n",
        "  print(\"Saved plot -> time_comparison_plot.png\")\n",
        "\n",
        "  return df\n",
        "\n",
        "sub_benchmark_speed()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOww0SNIh4A1"
      },
      "source": [
        "This is the part of the code to benchmark k-values vs ScaNN times. The num_queries are hardcoded and K_NEIGHBORS are a set. As said, if you have used GloVe-200, it is recommended you switch back to a lighter dataset to handle an insane amount of tree rebuilding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7l3gVkKyzY-z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import tempfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "import scann\n",
        "\n",
        "K_NEIGHBORS = [5, 10, 100, 500, 1000, 2000, 4000, 8000, 10000]\n",
        "TEST_QUERIES = 100\n",
        "\n",
        "def truth_recall(neighbors, true_neighbors):\n",
        "    \"\"\"\n",
        "    Computes recall @k by comparing the results of the approximate search\n",
        "    (neighbors) against the dataset's provided ground truth.\n",
        "    \"\"\"\n",
        "    total = 0\n",
        "    # Iterate through query results, comparing the approximate set against the true set\n",
        "    for gt_row, row in zip(true_neighbors, neighbors):\n",
        "        # Count the number of common elements (true positives)\n",
        "        total += np.intersect1d(gt_row, row).shape[0]\n",
        "\n",
        "    # Recall is (True Positives) / (Total True Neighbors)\n",
        "    return total / true_neighbors.size\n",
        "\n",
        "\n",
        "def benchmark_instance_speed_k(num_queries, k_neighbors, dataset_embeddings, test_embeddings, dataset_truth):\n",
        "  print(f\"Running benchmark with {num_queries} queries...\")\n",
        "\n",
        "  print(f\"Building ScaNN index for {k_neighbors} neighbors...\")\n",
        "\n",
        "  num_train = dataset_embeddings.shape[0]\n",
        "\n",
        "  num_leaves = max(int(np.sqrt(num_train) * 4), 100)  # rcm val: 4 * sqrt(num_hl);\n",
        "  num_leaves_to_search = max(int(num_leaves * 0.05), 10)  # requires tuning in order to get the desired recall value\n",
        "  training_sample_size = min(int(num_train * 0.3), num_train - 1)  # ~30% of dataset\n",
        "\n",
        "  searcher = scann.scann_ops_pybind.builder(\n",
        "      dataset_embeddings,\n",
        "      k_neighbors,\n",
        "      \"dot_product\"\n",
        "  ).tree(\n",
        "      num_leaves=num_leaves,\n",
        "      num_leaves_to_search=num_leaves_to_search,\n",
        "      training_sample_size=training_sample_size\n",
        "  ).score_ah(\n",
        "      dimensions_per_block=2,\n",
        "      anisotropic_quantization_threshold=0.2\n",
        "  ).reorder(k_neighbors * 10).build()\n",
        "\n",
        "  if num_queries > test_embeddings.shape[0]:\n",
        "    raise ValueError(f\"Not enough queries in the test embeddings! {num_queries} > {test_embeddings.shape[0]}\")\n",
        "\n",
        "  query_batch = test_embeddings[:num_queries]\n",
        "\n",
        "  sc_start = time.perf_counter()\n",
        "  scann_neighbors, _ = searcher.search_batched(query_batch)\n",
        "  sc_end = time.perf_counter()\n",
        "  sc_time = sc_end - sc_start\n",
        "\n",
        "  brute = scann.scann_ops_pybind.builder(\n",
        "      dataset_embeddings,\n",
        "      k_neighbors,\n",
        "      \"dot_product\"\n",
        "  ).score_brute_force().build()\n",
        "\n",
        "  # ---- Brute force ----\n",
        "  bf_start = time.perf_counter()\n",
        "  true_neighbors, _ = brute.search_batched(query_batch)\n",
        "  bf_end = time.perf_counter()\n",
        "  bf_time = bf_end - bf_start\n",
        "\n",
        "  # ---- Recall ----\n",
        "  recall_value = truth_recall(scann_neighbors, true_neighbors)\n",
        "\n",
        "  return {\n",
        "    \"recall@k\": recall_value,\n",
        "    \"scann_time_sec\": sc_time,\n",
        "    \"brute_force_time_sec\": bf_time,\n",
        "    \"speedup\": bf_time / sc_time if sc_time > 0 else np.inf\n",
        "  }\n",
        "\n",
        "def benchmark_k():\n",
        "\n",
        "  dataset_train_local = loaded_train_embeddings\n",
        "  dataset_test_local = loaded_test_embeddings\n",
        "  dataset_truth_local = loaded_truth_embedding\n",
        "\n",
        "  num_train = dataset_train_local.shape[0]\n",
        "\n",
        "  print(\"Dataset size: {num_train}\")\n",
        "  print(\"Loaded dataset embeddings.\")\n",
        "\n",
        "  # ===================================\n",
        "  # Run benchmarks\n",
        "  # ===================================\n",
        "  results = []\n",
        "\n",
        "  for k in K_NEIGHBORS:\n",
        "    stats = benchmark_instance_speed_k(TEST_QUERIES, k, dataset_train_local, dataset_test_local, dataset_truth_local)\n",
        "    results.append(stats)\n",
        "\n",
        "  df = pd.DataFrame(results)\n",
        "\n",
        "  print(\"\\n=== Benchmark Results ===\")\n",
        "  print(df)\n",
        "\n",
        "  # Save to CSV\n",
        "  df.to_csv(\"benchmark_k_results.csv\", index=False)\n",
        "  print(\"Saved CSV -> benchmark_k_results.csv\")\n",
        "\n",
        "  # ===================================\n",
        "  # Plotting\n",
        "  # ===================================\n",
        "\n",
        "  # ---- Query Time vs K ----\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(K_NEIGHBORS, df[\"scann_time_sec\"], marker='o', linewidth=2)\n",
        "  plt.plot(K_NEIGHBORS, df[\"brute_force_time_sec\"], label=\"Brute-force\", marker='o')\n",
        "  plt.xlabel(\"Number of neighbors (K)\")\n",
        "  plt.ylabel(\"Query time (seconds)\")\n",
        "  plt.title(f\"ScaNN query time vs K neighbors ({TEST_QUERIES} queries)\")\n",
        "  plt.grid(True)\n",
        "  plt.savefig(\"query_time_vs_k.png\")\n",
        "  print(\"Saved plot -> query_time_vs_k.png\")\n",
        "  plt.show()\n",
        "\n",
        "  return df\n",
        "\n",
        "benchmark_k()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}